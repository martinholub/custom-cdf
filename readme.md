---
title: "Generating Custom CDFs"
author: Martin Holub
date: Oct 30, 2018
output:  
  pdf_document:
    toc: true
highlight: zenburn
#  html_document:
#    embed_local_images: true
#    offline: true
#    toc: true
#separator: <!--s-->
#verticalSeparator: <!--v-->
#theme: white
#revealOptions:
#  transition: 'fade'
---

<!-- # Generating Custom CDFs
Martin Holub  
August 2, 2018 -->

This document describes the custom CDF workflow. The workflow is controlled by config which is documented in `configs/readme.md`, here important steps and choices are discussed. For documentation of individual rules, please consult their docstring in `pipe/Snakefile`.


# Easy Install
Install [miniconda](https://conda.io/miniconda.html) and run:
`conda env create --name cdf -f pipe/envs/cdf.yml`

# Quick Start

```bash
source activate cdf
cd pipe
snakemake -pr -j2 -d=../example --configfile=../example/example.yaml -s=Snakefile --use-conda -n
```
Note that this will fail on generating CDF because the example is too minimal. All previous rule should execute successfully.

### Flags
- `-pr`: print shell commands and reasons for execution
- `-j2`: use 2 threads
- `-n`: dry run

# Validation

Validation of generated `CDF`s or `ChipDb`s can be done in three ways.

1.  Install generated package and use it to process an experiment and inspect the QC report.
2. Follow approach in `scripts/R/r_tester.R` to compare the custom gene-probe annotation (as generated by this pipeline) with the one that was previously in use (available in `best_genelmeasure_<platform>_Gene.csv` file).
3. Import the newly processed experiment on new platform in Genevestigator and compare with the old experiment on old platform.

The recommended approach is to install the package, process an experiment and validate the produced values as described in step *2*.  

Both Agilent and Affymetrix custom CDF/ChipDB geenration procedures have been validated also with step *3*, each on one experiment. Overall same patterns of expression are seen and above 75% genes are preserved on the custom platform. About 90% of highly-expressed genes are common to both platforms although the FDR threshold may differ. Few novel transcripts emerge in the experiments processed with custom annotation package (built from newer references and genome annotations).

# Requirements

R dependencies are handled via separate *conda* environment. The packages are being sourced from bioconda's recipes. This approach is not yet fully mature (in worst case scenario, you may need to drop the `--use-conda` flag and install R packages defined in `deps/deps.txt`[^1] manually). Note that I had to make a pull request to the `AnnotationForge` package that was not yet accepted. Until this happens, you should be installing it [from my github](https://github.com/martinholub/AnnotationForge) (Else it will fail on creating a new organism for Agilent).

The prefered approach is to [contribute recipes](https://bioconda.github.io/contributing.html) to [bioconda channel](https://github.com/bioconda/bioconda-recipes) which will allow you to install missing packages to conda environment in consistent and portable manner. It is easy.

[^1]: These should be the non-base packages needed above the base *R 3.4.3*, they will fetch their dependencies as needed.

# Documentation

## Comments
Snakemake offers to flag rule *inputs* and *outputs* as `ancient` or `temp` respectively. This introduces relatively complex interplay in the Directed Acyclic Graph (DAG) of the workflow execution. Before these flags are pruned, you may need to manually delete some intermediate files to assure that they are recreated from updated inputs.


## Setting up the workflow
**rules:** `setup_workflow`, `unzip_data`, `merge_ref`, `normalize_probenames`, `unique_probes`

The workflow is setup automatically from specifications in *config*. Files are downloaded, checked for validity and repackaged and renamed if needed. Files are (g)unzipped as many tools require uncompressed input. You can use only subset of the genome as the reference (as e.g. separate files for select chromosomes), although using single file is preferred. Probenames may need to be normalized to common format and probes-FATSA file is subset to make sure that probes on input to alignment are unique.

## Alignment

**rules:** `bowtie_build`, `bowite_align`, `samtools_sort`

Before running the alignment, index must be built. We allow alignment to both strands (neither `--nofw` nor `--norc` flags used), report all valid alignments (`--all`) and control number of allowed mismatches (`-v {params.max_mismatches}` with 0 recommended value). The alignments are then converted to binary `BAM` file and sorted.

## SNP filtering

**rules:** `snp_transcriptome`, `snp_filter`, `samtools_index`, `bam_to_sam`

SNP filtering is optional (`variants: filter` option in *config*) and can be done for both alignments to genomic and transcriptomic references. For the transcriptomic reference, the SNV cooridnates must be first converted to coordinates along individual transcripts with `snp_transcriptome` rule.

The filtering is implemented in two steps. First, each alignment in `BAM` is trimmed by `clip` number of bases on both ends (e.g 5 for sequence that is 25bp long). Second, all trimmed alignments that have intersection with any SNV (which is 1bp long) are dropped from the `BAM` file of alignments. This results in alignments that do not have an overlap with known SNV location in their central part.

The `BAM` file is then indexed for faster access and must be also converted back to `SAM` format for access from python scripts that use the contained information in later steps.

<!-- TODO(mholub): Could access BAM from python? https://github.com/JohnLonginotto/pybam-->

## Feature Counting

**rules:** `count_features`

Counting is done on `feature` level and counts are aggregated on `metafeature` level. Approach differs between genomic and transcriptomic alignment. In particular, for *transcriptomic* alignment we do both counting and results extraction ourselves. For *genomic* alignment, the counting is done with `FeatureCounts` and only results are extracted by us (in consistence both in form and approach with transcriptomic results extraction).

The transcriptomic feature hit counting proceeds in several steps. First, the *lookup* between genes and transcripts is constructed from annotation file. Second, alignments are pulled from `SAM` file, reverse complementing their sequence if needed (as per combination of *strandedness* of the alignmnet and *sense* of the chip). Third, alignments (probes) are assigned to to both *genes* and *transcripts*, optionally filtered on multimappers on either level, probeset size. It is possible to blacklist some probes (e.g. controls) or whitelist some genes from being considered as targets of multimapping. Depending on the level of summarization either the *gene* or *transcript* level counts are passed to results extraction step.

For genomic feature hit counting, we pull out information from `SAM` produced by `featureCounts` and parse it into dictionary of alignments. Then we implement same filtering criteria as for transcriptomic feature counting.  

## Data Extraction

**rules:** `extract_data`

Data is extracted from the dictionary obtained in feature counting step. The data is parsed such as to be amenable to creation of `CDF` or `ChipDb` packages using dedicated `R` functions in later steps of the workflow. Whereas for Agilent (and probably also Illumina) we require just `probe <-> metafeature` mapping, Affymetrix additionally includes coordinates of the probe, its sequence and strandedness (which is identical for all probes on the chip). All can be parsed from the name of the probe as appearing in the FASTA file, as this complies to convention.

## CDF/ChipDB generation

**rules:** `make_cdf`, `make_annodb`

### CDF

The creation of CDF from properly formated data is straightforward and follows by first restructuring the data to correspond to CDF format specification, creating binary CDF and packaging it to an installable R package.

For high level overview of the procedure, see `doc/creatingR_CDFpackage.md`.

###  ChipDB

Thea creation of a ChipDB file from feature <-> probe mapping depends on availability of an organism annotation package. This can be either:
1. installed `OrgDb` package (available from [Bioconductor](https://bioconductor.org/packages/3.7/data/annotation/) and fetched automatically if needed),
2. or it can be represented as a `chipdbschema` (available ones can be listed with `AnnotationForge::available.chipdbschemas()`).
3. or new organism package can be generated from information obtained from `AnnotationHub`

Selection of options is done via *config*. Option *1* should be default for all organisms that have the package available. Option two requires translation of gene_ids to ENTREZID if not already which can be lossy. Additionally there are just few extra organisms available through this approach as compared to option *1* thus this is discouraged. Option *3* is the most general and should work for most organisms, even more exotic ones, provided that they are available in AnnotaitonHub.

Note that there is a tradeoff. You can either convert all your genes to `entrezgene`, which will then result in an `ChipDB` that is correctly joined on this `id` between probes and genes annotation. The conversion to `entrezgene` is however lossy. As we later convert all ids back to `ensemble_gene_id`, it is compelling to skip the conversion step. In such a case, gene annotation must be fetched via an intermediate conversion step (which is also lossy, but this hurts us much less because annotation is used just for duplicates).  

When working with with a reference/annotation with less common gene_ids, you probably want to convert between synonyms by specifying `in_id, out_id` pair (e.g. `in_id: ensembl_gene_id, out_id:  entrezgene`).

## Report

**rules:** `workflow_graph`, `report`

In the last step, `html` report is generated . The report is constructed with `report.Rmd` file using information pulled out from logs that are saved in `./logs` directory. This is not exactly robust, but quite straightforward. Note that the script producing report must be adapted if the pipeline changes to reflect thee changes.
